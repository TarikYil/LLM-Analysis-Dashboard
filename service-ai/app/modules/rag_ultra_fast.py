import uuid
import os
import numpy as np
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import asyncio
import concurrent.futures
from typing import List, Optional
import torch
import psycopg2
import psycopg2.extras
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time
import multiprocessing as mp
from functools import partial
import asyncpg
import json

# GPU desteÄŸi kontrol et
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸš€ Ultra Fast Embedding device: {device}")

# Model yÃ¼kleme (global)
model = None
def get_model(model_name: str = "fastest"):
    global model
    if model is None:
        FAST_MODELS = {
            "fastest": "all-MiniLM-L6-v2",
            "balanced": "all-mpnet-base-v2",
            "multilingual": "paraphrase-multilingual-MiniLM-L12-v2"
        }
        model_path = FAST_MODELS.get(model_name, FAST_MODELS["fastest"])
        print(f"ðŸ“¥ Loading ultra fast model: {model_path}")
        model = SentenceTransformer(model_path, device=device)
        print(f"âœ… Ultra fast model loaded on {device}")
    return model

# Gemini ayarÄ±
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# Database URL
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://service_user:service_pass123@localhost:5432/service_ai")

class UltraFastEmbeddingProcessor:
    def __init__(self, 
                 model_name: str = "fastest", 
                 batch_size: int = 64,  # Daha bÃ¼yÃ¼k batch
                 db_workers: int = 8,   # Paralel DB worker
                 db_batch_size: int = 5000):  # BÃ¼yÃ¼k DB batch
        self.model = get_model(model_name)
        self.batch_size = batch_size
        self.db_workers = db_workers
        self.db_batch_size = db_batch_size
        self.executor = ThreadPoolExecutor(max_workers=db_workers)
    
    def create_texts_from_df(self, df) -> List[str]:
        """DataFrame'den metinleri hÄ±zlÄ± oluÅŸtur"""
        texts = []
        for _, row in df.iterrows():
            text = f"Tarih:{row.get('SUBSCRIPTION_DATE', 'N/A')}, Ä°lÃ§e:{row.get('SUBSCRIPTION_COUNTY', 'N/A')}, Abone:{row.get('NUMBER_OF_SUBSCRIBER', 0)}"
            if 'SUBSCRIBER_DOMESTIC_FOREIGN' in row:
                text += f", Tip:{row['SUBSCRIBER_DOMESTIC_FOREIGN']}"
            texts.append(text)
        return texts
    
    def ultra_fast_encode(self, texts: List[str]) -> np.ndarray:
        """Ultra hÄ±zlÄ± GPU encoding"""
        print(f"ðŸš€ Ultra fast encoding {len(texts)} texts in batches of {self.batch_size}")
        start_time = time.time()
        
        embeddings = self.model.encode(
            texts, 
            batch_size=self.batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True,
            device=device
        )
        
        elapsed = time.time() - start_time
        print(f"âš¡ Ultra fast encoding completed in {elapsed:.2f}s ({len(texts)/elapsed:.1f} texts/sec)")
        return embeddings

def db_worker_insert(args):
    """Paralel database worker fonksiyonu"""
    worker_id, data_chunk, token, filename = args
    
    try:
        # Her worker kendi connection'Ä± kullanÄ±r
        conn = psycopg2.connect(DATABASE_URL)
        cur = conn.cursor()
        
        start_time = time.time()
        
        # Ã‡ok hÄ±zlÄ± bulk insert
        psycopg2.extras.execute_values(
            cur,
            """
            INSERT INTO documents (token, filename, content, embedding)
            VALUES %s
            """,
            data_chunk,
            template=None,
            page_size=2000,  # BÃ¼yÃ¼k page size
            fetch=False  # Return deÄŸeri almÄ±yoruz (daha hÄ±zlÄ±)
        )
        
        conn.commit()
        conn.close()
        
        elapsed = time.time() - start_time
        speed = len(data_chunk) / elapsed
        print(f"ðŸ’¾ Worker {worker_id}: {len(data_chunk)} kayÄ±t {elapsed:.2f}s'de kaydedildi ({speed:.0f} records/sec)")
        
        return len(data_chunk)
        
    except Exception as e:
        print(f"âŒ Worker {worker_id} hatasÄ±: {e}")
        if 'conn' in locals():
            conn.close()
        return 0

class UltraFastDatabaseInserter:
    def __init__(self, workers: int = 8, chunk_size: int = 5000):
        self.workers = workers
        self.chunk_size = chunk_size
    
    def parallel_bulk_insert(self, token: str, filename: str, texts: List[str], embeddings: np.ndarray) -> bool:
        """Paralel bulk insert ile ultra hÄ±zlÄ± kaydetme"""
        try:
            print(f"ðŸ”¥ Ultra fast parallel insert: {len(texts)} kayÄ±t, {self.workers} worker")
            start_time = time.time()
            
            # Veriyi chunk'lara bÃ¶l
            data_chunks = []
            for i in range(0, len(texts), self.chunk_size):
                chunk_texts = texts[i:i+self.chunk_size]
                chunk_embeddings = embeddings[i:i+self.chunk_size]
                
                chunk_data = [
                    (token, filename, text, embedding.tolist())
                    for text, embedding in zip(chunk_texts, chunk_embeddings)
                ]
                data_chunks.append(chunk_data)
            
            print(f"ðŸ“Š {len(data_chunks)} chunk oluÅŸturuldu, chunk baÅŸÄ±na ~{self.chunk_size} kayÄ±t")
            
            # Paralel iÅŸleme iÃ§in worker argÃ¼manlarÄ± hazÄ±rla
            worker_args = [
                (i, chunk, token, filename) 
                for i, chunk in enumerate(data_chunks)
            ]
            
            # ProcessPoolExecutor ile paralel insert
            with ProcessPoolExecutor(max_workers=self.workers) as executor:
                results = list(executor.map(db_worker_insert, worker_args))
            
            total_inserted = sum(results)
            elapsed = time.time() - start_time
            speed = total_inserted / elapsed
            
            print(f"ðŸŽ‰ Ultra fast insert tamamlandÄ±!")
            print(f"ðŸ“ˆ {total_inserted} kayÄ±t {elapsed:.2f}s'de kaydedildi")
            print(f"âš¡ Ortalama hÄ±z: {speed:.0f} records/sec")
            
            return total_inserted == len(texts)
            
        except Exception as e:
            print(f"âŒ Ultra fast insert hatasÄ±: {e}")
            return False

# Global processor instances
ultra_processor = UltraFastEmbeddingProcessor()
ultra_inserter = UltraFastDatabaseInserter()

async def ultra_fast_save_to_postgres(df, filename: str) -> Optional[str]:
    """Ultra hÄ±zlÄ± asenkron embedding ve kaydetme"""
    token = str(uuid.uuid4())
    
    print(f"ðŸš€ Ultra Fast: {len(df)} satÄ±r iÅŸleniyor")
    total_start = time.time()
    
    try:
        # 1. Metinleri oluÅŸtur (hÄ±zlÄ±)
        texts = ultra_processor.create_texts_from_df(df)
        if not texts:
            return None
        
        # 2. GPU ile ultra hÄ±zlÄ± embedding
        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            ultra_processor.executor, 
            ultra_processor.ultra_fast_encode, 
            texts
        )
        
        # 3. Paralel ultra hÄ±zlÄ± database insert
        success = await loop.run_in_executor(
            ultra_processor.executor,
            ultra_inserter.parallel_bulk_insert,
            token, filename, texts, embeddings
        )
        
        if success:
            total_time = time.time() - total_start
            total_speed = len(texts) / total_time
            print(f"ðŸŽ¯ ULTRA FAST TOPLAM: {len(texts)} kayÄ±t {total_time:.2f}s'de tamamlandÄ±")
            print(f"ðŸ† Genel hÄ±z: {total_speed:.0f} kayÄ±t/saniye")
            print(f"ðŸŽ« Token: {token}")
            return token
        else:
            return None
            
    except Exception as e:
        print(f"âŒ Ultra fast process hatasÄ±: {e}")
        return None

# AsyncPG ile daha hÄ±zlÄ± retrieval (opsiyonel)
async def ultra_fast_retrieve_context(token: str, question: str, top_k: int = 10) -> str:
    """Ultra hÄ±zlÄ± asenkron retrieval"""
    try:
        # Soruyu encode et
        loop = asyncio.get_event_loop()
        q_emb = await loop.run_in_executor(
            ultra_processor.executor,
            lambda: ultra_processor.model.encode([question])[0]
        )
        
        # AsyncPG ile hÄ±zlÄ± query
        conn = await asyncpg.connect(DATABASE_URL)
        
        rows = await conn.fetch("""
            SELECT content FROM documents
            WHERE token = $1
            ORDER BY embedding <=> $2::vector 
            LIMIT $3
        """, token, q_emb.tolist(), top_k)
        
        await conn.close()
        
        if not rows:
            return f"Token '{token}' iÃ§in veri bulunamadÄ±"
            
        return "\n".join([row['content'] for row in rows])
        
    except Exception as e:
        print(f"âŒ Ultra fast retrieval hatasÄ±: {e}")
        return f"Arama hatasÄ±: {str(e)}"

# Senkron wrapper'lar
def ultra_fast_save_sync(df, filename: str) -> Optional[str]:
    return asyncio.run(ultra_fast_save_to_postgres(df, filename))

def ultra_fast_retrieve_sync(token: str, question: str, top_k: int = 10) -> str:
    return asyncio.run(ultra_fast_retrieve_context(token, question, top_k))
